# -*- coding: utf-8 -*-
"""4c_학습.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/130VkcoamYqrTJcrRnldB_aJ8OtupVv0V
"""

from google.colab import drive
drive.mount('/content/drive')

import os
goddess_dir = os.path.join("/content/drive/MyDrive/New/data/goddess")
tree_dir = os.path.join('/content/drive/MyDrive/New/data/tree')
warrior2_dir = os.path.join('/content/drive/MyDrive/New/data/warrior2')
false_dir = os.path.join('/content/drive/MyDrive/New/data/false')

goddess_files = os.listdir(goddess_dir)
tree_files = os.listdir(tree_dir)
warrior2_files = os.listdir(warrior2_dir)
false_files = os.listdir(false_dir)

print('Total number of training goddess images:', len(goddess_files))
print('Total number of training tree images:', len(tree_files))
print('Total number of training warrior2 images:', len(warrior2_files))
print('Total number of training false images:', len(false_files))

print(goddess_files[:10])
print(tree_files[:10])
print(warrior2_files[:10])
print(false_files[:10])

import matplotlib.pyplot as plt
import matplotlib.image as mpimg

pic_index = 2

next_goddess = [os.path.join(goddess_dir, fname) for fname in goddess_files[pic_index-2:pic_index]]
next_tree = [os.path.join(tree_dir, fname) for fname in tree_files[pic_index-2:pic_index]]
next_warrior2 = [os.path.join(warrior2_dir, fname) for fname in warrior2_files[pic_index-2:pic_index]]
next_false = [os.path.join(false_dir, fname) for fname in false_files[pic_index-2:pic_index]]

for i, img_path in enumerate(next_goddess + next_tree + next_warrior2+next_false):
  print(img_path)
  img = mpimg.imread(img_path)
  plt.imshow(img)
  plt.axis('Off')
  plt.show()

from glob import glob
import cv2
import numpy as np
from PIL import Image

caltech_dir = os.path.join("/content/drive/MyDrive/New/data")
categories = ["goddess", "tree", "warrior2","false"]
nb_classes = len(categories)

image_w = 71 
image_h = 71

pixels = image_h * image_w * 3

X = []
y = []

for idx, cat in enumerate(categories):
    
    #one-hot encoding
    label = [0 for i in range(nb_classes)]
    label[idx] = 1

    image_dir = caltech_dir + "/" + cat
    files = glob(image_dir+"/*.jpg")
    
    for i, f in enumerate(files):
        img = Image.open(f)
        img = img.convert("RGB")
        img = img.resize((image_w, image_h))
        data = np.asarray(img)
        
        X.append(data)
        y.append(label)

        
        if i % 1000 == 0:
            print(cat, " : ", f)

from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import matplotlib.pyplot as plt
#import keras.backend.tensorflow_backend as K
import tensorflow as tf
from tensorflow.keras import regularizers
from tensorflow.keras import layers, models
from tensorflow.keras.applications import VGG16
from tensorflow.keras import Input
from tensorflow.keras.models import Model
from tensorflow.keras import optimizers, initializers, regularizers, metrics
from tensorflow.keras.callbacks import ModelCheckpoint
import os
from glob import glob
from PIL import Image
import numpy as np

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

X = np.array(X)
y = np.array(y)

X_train, X_test, y_train, y_test = train_test_split(X, y)
xy = (X_train, X_test, y_train, y_test)

#일반화
X_train = X_train.astype(float) / 255
X_test = X_test.astype(float) / 255

import tensorflow.compat.v1 as tf

#vgg16 모델 불러오기
pre_trained_vgg = VGG16(weights='imagenet', include_top=False,input_shape=(71, 71, 3))

pre_trained_vgg.trainable =False
pre_trained_vgg.summary()

#vgg16 밑에 레이어 추가
with tf.device('/device:GPU:0'):
  additional_model = models.Sequential() 
  additional_model.add(pre_trained_vgg)
  additional_model.add(layers.Flatten())
  additional_model.add(layers.Dense(4096, kernel_regularizer = regularizers.l1_l2
                                      (l1=0.001,l2=0.001),activation='relu'))
  additional_model.add(layers.Dropout(0.5))
  additional_model.add(layers.Dense(2048, kernel_regularizer = regularizers.l1_l2
                                   (l1=0.001,l2=0.001),activation='relu'))
  additional_model.add(layers.Dropout(0.5))
  additional_model.add(layers.Dense(1024, kernel_regularizer = regularizers.l1_l2
                                   (l1=0.001,l2=0.001),activation='relu'))
  additional_model.add(layers.Dropout(0.5))
  additional_model.add(layers.Dense(4, activation='sigmoid'))

  additional_model.compile(loss='categorical_crossentropy',
                  optimizer=optimizers.RMSprop(lr=1e-4),
                  metrics=['acc'])
  history = additional_model.fit(X_train, y_train, 
                    batch_size=2, 
                    epochs=40, 
                    validation_data=(X_test, y_test))

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'go', label='Training acc')
plt.plot(epochs, val_acc, 'g', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'go', label='Training loss')
plt.plot(epochs, val_loss, 'g', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

model_json = additional_model.to_json()
with open("model.json", "w") as json_file : 
    json_file.write(model_json)

# SavedModel로 전체 모델을 저장합니다
!mkdir -p saved_model
additional_model.save('/content/drive/MyDrive/Project/model_r.h5') 
print("Saved model to disk")